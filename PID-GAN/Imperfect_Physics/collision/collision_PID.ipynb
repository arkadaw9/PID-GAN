{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pylab as py\n",
    "import time\n",
    "import cv2\n",
    "import scipy.io as spio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture 1\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim=40, num_layers=5):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        block_layer = block(in_dim, hid_dim)\n",
    "        for layer in block_layer:\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        for layer_i in range(num_layers - 1):\n",
    "            block_layer = block(hid_dim, hid_dim)\n",
    "            for layer in block_layer:\n",
    "                self.layers.append(layer)\n",
    "                \n",
    "        self.layers.append(nn.Linear(hid_dim, out_dim))\n",
    "        \n",
    "    def forward(self, data, noise):\n",
    "        # Concatenate input data and noise to produce output\n",
    "        gen_input = torch.cat((data, noise), -1)\n",
    "        \n",
    "        out = gen_input\n",
    "        for i in range(len(self.layers)):\n",
    "            out = self.layers[i](out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim = 1, hid_dim=20, num_layers=2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.layers = [nn.Linear(in_dim, hid_dim)]\n",
    "        self.layers.append(nn.Dropout(0.4))\n",
    "        self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim = 1\n",
    "        \n",
    "        for layer_i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hid_dim, hid_dim))\n",
    "            self.layers.append(nn.Dropout(0.4))\n",
    "            self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            \n",
    "        self.layers.append(nn.Linear(hid_dim, self.out_dim))\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, data, out, phy):\n",
    "        # Concatenate data and out to produce input\n",
    "        d_in = torch.cat((data, out, phy), -1)\n",
    "        d_out = self.model(d_in)\n",
    "        return d_out\n",
    "    \n",
    "    \n",
    "# x,y -> z\n",
    "class Q_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_Net, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *block(data_dim + out_dim, 20, normalize=False),\n",
    "            *block(20, 20),\n",
    "            nn.Linear(20, noise_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, output):\n",
    "        # Concatenate input data and noise to produce output\n",
    "        gen_input = torch.cat((data, output), -1)\n",
    "        noise = self.model(gen_input)\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Real = 0, fake = 1\n",
    "def discriminator_loss(logits_real, logits_fake, logits_fake_unlabelled):\n",
    "    \n",
    "    loss = - torch.mean(torch.log(1.0 - torch.sigmoid(logits_real) + 1e-8) + torch.log(torch.sigmoid(logits_fake) + 1e-8)) \\\n",
    "            - torch.mean(torch.log(torch.sigmoid(logits_fake_unlabelled) + 1e-8))\n",
    "    return loss\n",
    "\n",
    "def generator_loss(logits_fake, logits_fake_unlabelled):\n",
    "    \n",
    "    loss = torch.mean(logits_fake) + torch.mean(logits_fake_unlabelled)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(x, y, stat_x = [0,1], stat_y = [0,1]):  #stat [0] = mean, stat [1] = std\n",
    "    stat_x = torch.Tensor(stat_x).to(device)\n",
    "    stat_y = torch.Tensor(stat_y).to(device)\n",
    "    x = x * stat_x[1] + stat_x[0]\n",
    "    y = y * stat_y[1] + stat_y[0]\n",
    "    energy_loss =   - 0.5*x[:,4:5]*torch.pow(x[:,0:1],2) \\\n",
    "                    - 0.5*x[:,5:6]*torch.pow(x[:,1:2],2) \\\n",
    "                    + 0.5*x[:,4:5]*torch.pow(y[:,0:1],2) \\\n",
    "                    + 0.5*x[:,5:6]*torch.pow(y[:,1:2],2)\n",
    "                  \n",
    "    \n",
    "    momentum_loss =  - x[:,4:5] * x[:,0:1] \\\n",
    "                     - x[:,5:6] * x[:,1:2] \\\n",
    "                     + x[:,4:5] * y[:,0:1] \\\n",
    "                     + x[:,5:6] * y[:,1:2] \n",
    "    f = torch.cat([energy_loss, momentum_loss], dim = 1)\n",
    "    return f\n",
    "\n",
    "def expo_transformation(lambda_phy, phyloss):\n",
    "    probs = torch.exp(-lambda_phy * torch.abs(phyloss))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim, mean=0, std=1):\n",
    "    to_return = mean + std * torch.randn((batch_size, dim))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_frac = 0.8\n",
    "#load data\n",
    "data = np.loadtxt( '../../datasets/collision_shuffled.txt' )\n",
    "labels = data[:,-2:]\n",
    "x = data[:,:-2]\n",
    "\n",
    "\n",
    "#training and test splits\n",
    "n_obs = int(tr_frac * x.shape[0])\n",
    "train_x , train_y = x[:n_obs,:] , labels[:n_obs,:] \n",
    "test_x , test_y = x[n_obs:,:] , labels[n_obs:,:] \n",
    "\n",
    "#defining noise dimensions\n",
    "noise_dim = 2\n",
    "data_dim = train_x.shape[-1]\n",
    "out_dim = labels.shape[-1]\n",
    "\n",
    "# Normalization \n",
    "\n",
    "# train_x:\n",
    "mean_x = train_x.mean(axis=0)\n",
    "std_x = train_x.std(axis=0)\n",
    "\n",
    "train_x = (train_x-mean_x)/std_x\n",
    "test_x = (test_x-mean_x)/std_x\n",
    "\n",
    "# train_y:\n",
    "mean_y = train_y.mean(axis=0)\n",
    "std_y = train_y.std(axis=0)\n",
    "\n",
    "train_y = (train_y-mean_y)/std_y\n",
    "test_y = (test_y-mean_y)/std_y\n",
    "\n",
    "#defining batch size and parameters\n",
    "batch_size = 64 # mini-batch size\n",
    "num_workers = 4 # how many parallel workers are we gonna use for reading data\n",
    "shuffle = True # shuffle the dataset\n",
    "\n",
    "#training and testing dataset creation\n",
    "train_x = torch.FloatTensor(train_x).to(device)\n",
    "test_x = torch.FloatTensor(test_x).to(device)\n",
    "train_y = torch.FloatTensor(train_y).to(device)\n",
    "test_y = torch.FloatTensor(test_y).to(device)\n",
    "\n",
    "train_loader = DataLoader(list(zip(train_x,train_y)), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "#DEFINING MODELS : GENERATOR, DISCRIMINATOR AND Q-NET\n",
    "\n",
    "D = Discriminator(in_dim = (data_dim + out_dim + 2), out_dim = 1, hid_dim=20, num_layers=2).to(device)\n",
    "G = Generator(in_dim = (noise_dim + data_dim), out_dim = out_dim, hid_dim=40, num_layers=5).to(device)\n",
    "Q = Q_Net().to(device)\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=1e-3, betas = (0.5, 0.999))\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=1e-3, betas = (0.5, 0.999))\n",
    "Q_optimizer = torch.optim.Adam(Q.parameters(), lr=1e-3, betas = (0.5, 0.999))\n",
    "\n",
    "###############################################################################################\n",
    "######################################  TRAINING PARAMETERS ###################################\n",
    "\n",
    "num_epochs = 5000\n",
    "lambda_mse = 0\n",
    "lambda_phy = 0\n",
    "lambda_prob = 0.001\n",
    "lambda_q = 0.5\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "Adv_loss = np.zeros(num_epochs)\n",
    "G_loss = np.zeros(num_epochs)\n",
    "D_loss = np.zeros(num_epochs)\n",
    "Q_loss = np.zeros(num_epochs)\n",
    "MSE_loss = np.zeros(num_epochs)\n",
    "PHY_loss = np.zeros(num_epochs)\n",
    "\n",
    "G_loss_batch = []\n",
    "D_loss_batch = []\n",
    "\n",
    "train_pred = np.zeros((num_epochs,train_y.shape[0],out_dim))\n",
    "test_pred = np.zeros((num_epochs,test_y.shape[0],out_dim))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    ## make directory to store results\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "\n",
    "        # DISCRIMINATOR UPDATE\n",
    "        D_optimizer.zero_grad()\n",
    "\n",
    "        ## REAL DATA \n",
    "        real_prob = expo_transformation(lambda_prob, physics_loss(x,y, [mean_x, std_x], [mean_y, std_y]))\n",
    "        real_logits = D.forward(x, y, real_prob)\n",
    "\n",
    "        ## PREDICTED SAMPLES FROM BATCH\n",
    "        D_noise = sample_noise(x.shape[0], noise_dim).to(device)\n",
    "        y_pred = G.forward(x, D_noise)\n",
    "        phy_prob = expo_transformation(lambda_prob, physics_loss(x,y_pred, [mean_x, std_x], [mean_y, std_y]))\n",
    "        fake_logits = D.forward(x, y_pred, phy_prob)\n",
    "\n",
    "        ## UNLABELLED DATA SAMPLES\n",
    "        D_noise_unlabelled = sample_noise(test_x.shape[0], noise_dim).to(device)\n",
    "        y_pred_unlabelled = G.forward(test_x, D_noise_unlabelled)\n",
    "        phy_prob_unlabelled = expo_transformation(lambda_prob, physics_loss(test_x,y_pred_unlabelled, [mean_x, std_x], [mean_y, std_y]))\n",
    "        fake_logits_unlabelled = D.forward(test_x, y_pred_unlabelled, phy_prob_unlabelled) \n",
    "\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits, fake_logits_unlabelled)\n",
    "\n",
    "\n",
    "        d_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # GENERATOR UPDATE\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        G_noise = sample_noise(x.shape[0], noise_dim).to(device)\n",
    "        y_pred = G.forward(x, G_noise)\n",
    "        phy_prob = expo_transformation(lambda_prob, physics_loss(x,y_pred, [mean_x, std_x], [mean_y, std_y]))\n",
    "        fake_logits = D.forward(x, y_pred, phy_prob)\n",
    "\n",
    "        ## UNLABELLED DATA SAMPLES\n",
    "        G_noise_unlabelled = sample_noise(test_x.shape[0], noise_dim).to(device)\n",
    "        y_pred_unlabelled = G.forward(test_x, G_noise_unlabelled)\n",
    "        phy_prob_unlabelled = expo_transformation(lambda_prob, physics_loss(test_x,y_pred_unlabelled, [mean_x, std_x], [mean_y, std_y]))\n",
    "\n",
    "        n_phy_tot = torch.cat([phy_prob_unlabelled, phy_prob], dim = 0)\n",
    "\n",
    "        fake_logits_unlabelled = D.forward(test_x, y_pred_unlabelled, phy_prob_unlabelled) \n",
    "\n",
    "        mse_loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "        phy_loss = torch.mean(torch.abs(physics_loss(x,y_pred, [mean_x, std_x], [mean_y, std_y])))  ### can also compute physics loss on the entire set.\n",
    "\n",
    "        z_pred = Q.forward(x, y_pred)\n",
    "        mse_loss_Z = torch.nn.functional.mse_loss(z_pred, G_noise)\n",
    "\n",
    "        adv_loss = generator_loss(fake_logits, fake_logits_unlabelled) \n",
    "        g_loss = adv_loss + lambda_mse * mse_loss + lambda_phy * phy_loss + lambda_q * mse_loss_Z\n",
    "\n",
    "        g_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        Q_optimizer.zero_grad()\n",
    "        Q_noise = sample_noise(x.shape[0], noise_dim).to(device)\n",
    "        y_pred = G.forward(x, Q_noise)\n",
    "        z_pred = Q.forward(x, y_pred)\n",
    "        q_loss = torch.nn.functional.mse_loss(z_pred, Q_noise)\n",
    "        q_loss.backward()\n",
    "        Q_optimizer.step()\n",
    "\n",
    "        G_loss_batch.append(g_loss.detach().cpu().numpy())\n",
    "        D_loss_batch.append(d_loss.detach().cpu().numpy())\n",
    "\n",
    "        Adv_loss[epoch] += adv_loss.detach().cpu().numpy()\n",
    "        MSE_loss[epoch] += mse_loss.detach().cpu().numpy()\n",
    "        G_loss[epoch] += g_loss.detach().cpu().numpy()\n",
    "        D_loss[epoch] += d_loss.detach().cpu().numpy()\n",
    "        Q_loss[epoch] += q_loss.detach().cpu().numpy()\n",
    "        PHY_loss[epoch] += phy_loss.detach().cpu().numpy()\n",
    "\n",
    "    if (epoch % 100 == 0):\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [MSE loss: %f] [G loss: %f] [D loss: %f] [Q loss: %f] [Phy loss: %f] [Adv G loss: %f]\"\n",
    "            % (epoch, num_epochs, MSE_loss[epoch], G_loss[epoch], D_loss[epoch], Q_loss[epoch], PHY_loss[epoch], Adv_loss[epoch] )\n",
    "        )\n",
    "\n",
    "    G_train_noise = sample_noise(train_x.shape[0], noise_dim).to(device)\n",
    "    train_pred[epoch,:,:] = G.forward(train_x, G_train_noise).detach().cpu().numpy()\n",
    "\n",
    "    G_test_noise = sample_noise(test_x.shape[0], noise_dim).to(device)\n",
    "    test_pred[epoch,:,:] = G.forward(test_x, G_test_noise).detach().cpu().numpy()\n",
    "\n",
    "# ###############################################################################################\n",
    "# ######################################## LOSS PLOTS ###########################################\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(Adv_loss)\n",
    "plt.plot(MSE_loss)\n",
    "plt.plot(G_loss)\n",
    "plt.plot(D_loss)\n",
    "plt.plot(PHY_loss)\n",
    "plt.legend(['Adv_loss','MSE_loss','G_loss','D_loss','PHY_loss'])\n",
    "#     plt.savefig(\"Vizualization/\"+str(experiment_name)+\"Loss_plot.jpg\")\n",
    "plt.show()\n",
    "# ###############################################################################################\n",
    "\n",
    "# ###############################################################################################\n",
    "# ################################## TEST PREDICTIONS ###########################################\n",
    "\n",
    "n_samples = 10000\n",
    "test_samples = np.zeros((n_samples, test_y.shape[0], test_y.shape[1]))\n",
    "print(test_samples.shape)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    G_test_noise = sample_noise(test_x.shape[0], noise_dim).to(device)\n",
    "    test_samples[i,:,:] = G.forward(test_x, G_test_noise).detach().cpu().numpy()*std_y+mean_y\n",
    "\n",
    "\n",
    "test_mean_y_0 = np.mean(test_samples, 0)[:, 0].flatten()\n",
    "test_mean_y_1 = np.mean(test_samples, 0)[:, 1].flatten()\n",
    "\n",
    "test_std_y_0 = np.std(test_samples, 0)[:, 0].flatten()\n",
    "test_std_y_1 = np.std(test_samples, 0)[:, 1].flatten()\n",
    "\n",
    "# print(test_y.shape)\n",
    "test_y_true_0 = test_y[:, 0].detach().cpu().numpy()*std_y[0] + mean_y[0]\n",
    "test_y_true_1 = test_y[:, 1].detach().cpu().numpy()*std_y[1] + mean_y[1]\n",
    "\n",
    "x = np.linspace(0, test_y.shape[0]-1, test_y.shape[0])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(x, test_mean_y_0 , label = 'test predictions', alpha= 0.9, color='b', marker='+')\n",
    "plt.fill_between(x, test_mean_y_0-2*test_std_y_0, test_mean_y_0+2*test_std_y_0, alpha=0.2, color='b')\n",
    "# plt.errorbar(x,test_mean_y_0,test_std_y_0)\n",
    "plt.plot(x, test_y_true_0, label = 'ground truth', alpha=1, color='r', marker='*')\n",
    "py.legend(loc='upper right')\n",
    "#     plt.savefig(\"Vizualization/\"+str(experiment_name)+\"Test_predictions_1.jpg\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(x, test_mean_y_1 , label = 'test predictions', alpha= 0.9, color='b', marker='+')\n",
    "plt.fill_between(x, test_mean_y_1-2*test_std_y_1, test_mean_y_1+2*test_std_y_1, alpha=0.2, color='b')\n",
    "# plt.errorbar(x,mean_y,std_y)\n",
    "plt.plot(x, test_y_true_1, label = 'ground truth', alpha=1, color='r', marker='*')\n",
    "py.legend(loc='upper right')\n",
    "#     plt.savefig(\"Vizualization/\"+str(experiment_name)+\"Test_predictions_2.jpg\")\n",
    "plt.show()\n",
    "\n",
    "test_x = test_x.detach().cpu().numpy()\n",
    "test_x = test_x * std_x + mean_x\n",
    "\n",
    "test_rmse0 = (((test_mean_y_0.flatten() - test_y_true_0.flatten())**2).mean())**0.5\n",
    "test_rmse1 = (((test_mean_y_1.flatten() - test_y_true_1.flatten())**2).mean())**0.5\n",
    "# test_rmse = (((np.stack(test_mean_y_0,test_mean_y_1) - test_y_true_1.flatten())**2).mean())**0.5\n",
    "\n",
    "energy_loss = np.mean(np.absolute(0.5*test_x[:,4]*np.power(test_x[:,0],2)+0.5*test_x[:,5]*np.power(test_x[:,1],2)\n",
    "                                  -0.5*test_x[:,4]*np.power(test_mean_y_0,2)-0.5*test_x[:,5]*np.power(test_mean_y_1,2)))\n",
    "\n",
    "momentum_loss = np.mean(np.absolute(test_x[:,4]*test_x[:,0] + test_x[:,5]*test_x[:,1] \n",
    "                                     - test_x[:,4]*test_mean_y_0 - test_x[:,5]*test_mean_y_1))\n",
    "test_phy_loss = (energy_loss + momentum_loss)\n",
    "\n",
    "test_true = np.stack((test_y_true_0, test_y_true_1), axis =-1).flatten()\n",
    "test_mean_y = np.stack((test_mean_y_0, test_mean_y_1), axis =-1).flatten()\n",
    "test_rmse = ((( test_mean_y - test_true)**2).mean())**0.5\n",
    "\n",
    "print(\"test RMSE = %f\" %(test_rmse))\n",
    "print(\"test RMSE va = %f\" %(test_rmse0))\n",
    "print(\"test RMSE vb = %f\" %(test_rmse1))\n",
    "print(\"test Physical Inconsistency = %f\" %(test_phy_loss))\n",
    "\n",
    "\n",
    "energy_loss_true = np.mean(np.absolute(0.5*test_x[:,4]*np.power(test_x[:,0],2)+0.5*test_x[:,5]*np.power(test_x[:,1],2)-0.5*test_x[:,4]*np.power(test_y_true_0,2)-0.5*test_x[:,5]*np.power(test_y_true_1,2)))\n",
    "\n",
    "momentum_loss_true = np.mean(np.absolute(test_x[:,4]*test_x[:,0] + test_x[:,5]*test_x[:,1] \n",
    "                                     - test_x[:,4]*test_y_true_0 - test_x[:,5]*test_y_true_1))\n",
    "test_phy_loss_true = (energy_loss_true + momentum_loss_true)\n",
    "print(\"test Physical Inconsistency (TRUE)= %f\" %(test_phy_loss_true))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

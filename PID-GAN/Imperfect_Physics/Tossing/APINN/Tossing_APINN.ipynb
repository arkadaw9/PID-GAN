{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pylab as py\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import warnings\n",
    "sys.path.insert(0, '../../../Scripts/')\n",
    "from models_imperfect import PGNN\n",
    "from apinn import *\n",
    "# from ../Scripts/helper import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:3')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "lambda_mse = 1\n",
    "tr_frac = 0.4\n",
    "\n",
    "#architecture for the models\n",
    "net_hid_dim = 50\n",
    "net_num_layer = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../../../datasets/tossing_trajectories.txt')\n",
    "x = data[:, 0:6]\n",
    "labels = data[:, 6:]\n",
    "\n",
    "# training and test split\n",
    "n_obs = int(tr_frac * x.shape[0])\n",
    "train_x , train_y = x[:n_obs,:] , labels[:n_obs, :] \n",
    "test_x , test_y = x[n_obs:,:] , labels[n_obs:, :]\n",
    "\n",
    "data_dim = train_x.shape[1]\n",
    "out_dim = labels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PGNN(in_dim = data_dim, out_dim = out_dim, hid_dim=net_hid_dim, num_layers=net_num_layer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "APINN = Tossing_APINN(train_x, train_y, test_x, test_y, net, device, num_epochs, lambda_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10000] [MSE loss: 0.996759] [Phy loss: 110.899002] [Total loss: 273.497559] [Lambda mse : 31.392628]\n",
      "[Epoch 100/10000] [MSE loss: 0.081032] [Phy loss: 10.001344] [Total loss: 11.732128] [Lambda mse : 13.531187]\n",
      "[Epoch 200/10000] [MSE loss: 0.063477] [Phy loss: 7.166821] [Total loss: 10.235534] [Lambda mse : 41.732868]\n",
      "[Epoch 300/10000] [MSE loss: 0.060130] [Phy loss: 5.679905] [Total loss: 8.322547] [Lambda mse : 20.258808]\n",
      "[Epoch 400/10000] [MSE loss: 0.053667] [Phy loss: 5.642215] [Total loss: 8.071356] [Lambda mse : 21.665174]\n",
      "[Epoch 500/10000] [MSE loss: 0.092520] [Phy loss: 5.058714] [Total loss: 7.626081] [Lambda mse : 20.384583]\n",
      "[Epoch 600/10000] [MSE loss: 0.048251] [Phy loss: 5.072522] [Total loss: 6.967111] [Lambda mse : 20.813732]\n",
      "[Epoch 700/10000] [MSE loss: 0.048751] [Phy loss: 4.611448] [Total loss: 6.481643] [Lambda mse : 26.241053]\n",
      "[Epoch 800/10000] [MSE loss: 0.080445] [Phy loss: 4.711361] [Total loss: 6.772224] [Lambda mse : 14.458344]\n",
      "[Epoch 900/10000] [MSE loss: 0.060139] [Phy loss: 4.673591] [Total loss: 7.447071] [Lambda mse : 24.948008]\n",
      "[Epoch 1000/10000] [MSE loss: 0.060249] [Phy loss: 4.720302] [Total loss: 6.688059] [Lambda mse : 14.221117]\n",
      "[Epoch 1100/10000] [MSE loss: 0.056866] [Phy loss: 4.455399] [Total loss: 6.314991] [Lambda mse : 35.623264]\n",
      "[Epoch 1200/10000] [MSE loss: 0.047981] [Phy loss: 4.381654] [Total loss: 5.805033] [Lambda mse : 17.892244]\n",
      "[Epoch 1300/10000] [MSE loss: 0.061123] [Phy loss: 4.587366] [Total loss: 7.272150] [Lambda mse : 16.993443]\n",
      "[Epoch 1400/10000] [MSE loss: 0.048065] [Phy loss: 4.476543] [Total loss: 6.116039] [Lambda mse : 29.823326]\n",
      "[Epoch 1500/10000] [MSE loss: 0.044436] [Phy loss: 4.385986] [Total loss: 6.567222] [Lambda mse : 43.436630]\n",
      "[Epoch 1600/10000] [MSE loss: 0.048472] [Phy loss: 4.206990] [Total loss: 5.625700] [Lambda mse : 34.165375]\n",
      "[Epoch 1700/10000] [MSE loss: 0.038861] [Phy loss: 4.128892] [Total loss: 5.513950] [Lambda mse : 45.624477]\n",
      "[Epoch 1800/10000] [MSE loss: 0.055343] [Phy loss: 4.227414] [Total loss: 6.421238] [Lambda mse : 23.452488]\n",
      "[Epoch 1900/10000] [MSE loss: 0.046543] [Phy loss: 4.246669] [Total loss: 6.606673] [Lambda mse : 24.419613]\n",
      "[Epoch 2000/10000] [MSE loss: 0.046034] [Phy loss: 4.108049] [Total loss: 5.740539] [Lambda mse : 17.262409]\n",
      "[Epoch 2100/10000] [MSE loss: 0.052313] [Phy loss: 4.243875] [Total loss: 5.595920] [Lambda mse : 10.991589]\n",
      "[Epoch 2200/10000] [MSE loss: 0.052661] [Phy loss: 4.349587] [Total loss: 6.783365] [Lambda mse : 17.623661]\n",
      "[Epoch 2300/10000] [MSE loss: 0.041280] [Phy loss: 4.129658] [Total loss: 5.908720] [Lambda mse : 30.694521]\n",
      "[Epoch 2400/10000] [MSE loss: 0.035103] [Phy loss: 4.179500] [Total loss: 5.536451] [Lambda mse : 51.361851]\n",
      "[Epoch 2500/10000] [MSE loss: 0.031244] [Phy loss: 4.058427] [Total loss: 5.320901] [Lambda mse : 45.529041]\n",
      "[Epoch 2600/10000] [MSE loss: 0.034540] [Phy loss: 3.829295] [Total loss: 4.948757] [Lambda mse : 32.756618]\n",
      "[Epoch 2700/10000] [MSE loss: 0.057007] [Phy loss: 4.224418] [Total loss: 5.894695] [Lambda mse : 14.062936]\n",
      "[Epoch 2800/10000] [MSE loss: 0.044072] [Phy loss: 4.150965] [Total loss: 5.430424] [Lambda mse : 11.769304]\n",
      "[Epoch 2900/10000] [MSE loss: 0.037079] [Phy loss: 4.052826] [Total loss: 5.226701] [Lambda mse : 17.139683]\n",
      "[Epoch 3000/10000] [MSE loss: 0.037116] [Phy loss: 3.967980] [Total loss: 5.165693] [Lambda mse : 25.532661]\n",
      "[Epoch 3100/10000] [MSE loss: 0.031857] [Phy loss: 4.242774] [Total loss: 5.843925] [Lambda mse : 74.836975]\n",
      "[Epoch 3200/10000] [MSE loss: 0.053645] [Phy loss: 3.855909] [Total loss: 5.390245] [Lambda mse : 22.562387]\n",
      "[Epoch 3300/10000] [MSE loss: 0.042287] [Phy loss: 3.943269] [Total loss: 5.253793] [Lambda mse : 39.986229]\n",
      "[Epoch 3400/10000] [MSE loss: 0.039363] [Phy loss: 3.767180] [Total loss: 5.427782] [Lambda mse : 17.564571]\n",
      "[Epoch 3500/10000] [MSE loss: 0.069388] [Phy loss: 3.996922] [Total loss: 5.950468] [Lambda mse : 13.989467]\n",
      "[Epoch 3600/10000] [MSE loss: 0.049094] [Phy loss: 3.963900] [Total loss: 5.617845] [Lambda mse : 22.162470]\n",
      "[Epoch 3700/10000] [MSE loss: 0.050639] [Phy loss: 3.755865] [Total loss: 5.256401] [Lambda mse : 32.141914]\n",
      "[Epoch 3800/10000] [MSE loss: 0.040880] [Phy loss: 3.962276] [Total loss: 5.391070] [Lambda mse : 19.003826]\n",
      "[Epoch 3900/10000] [MSE loss: 0.048531] [Phy loss: 4.089403] [Total loss: 5.832098] [Lambda mse : 15.716429]\n",
      "[Epoch 4000/10000] [MSE loss: 0.044136] [Phy loss: 3.982613] [Total loss: 5.570046] [Lambda mse : 22.503841]\n",
      "[Epoch 4100/10000] [MSE loss: 0.037790] [Phy loss: 4.066700] [Total loss: 5.797612] [Lambda mse : 15.492157]\n",
      "[Epoch 4200/10000] [MSE loss: 0.033765] [Phy loss: 3.986882] [Total loss: 5.138333] [Lambda mse : 33.421448]\n",
      "[Epoch 4300/10000] [MSE loss: 0.048061] [Phy loss: 3.809404] [Total loss: 5.375492] [Lambda mse : 28.964550]\n",
      "[Epoch 4400/10000] [MSE loss: 0.051214] [Phy loss: 4.006362] [Total loss: 5.560624] [Lambda mse : 12.702152]\n",
      "[Epoch 4500/10000] [MSE loss: 0.036350] [Phy loss: 3.826446] [Total loss: 4.851784] [Lambda mse : 21.627497]\n",
      "[Epoch 4600/10000] [MSE loss: 0.053512] [Phy loss: 3.951562] [Total loss: 5.365348] [Lambda mse : 9.487144]\n",
      "[Epoch 4700/10000] [MSE loss: 0.034781] [Phy loss: 3.807424] [Total loss: 5.053602] [Lambda mse : 18.757700]\n",
      "[Epoch 4800/10000] [MSE loss: 0.044802] [Phy loss: 3.935233] [Total loss: 5.451164] [Lambda mse : 13.865758]\n",
      "[Epoch 4900/10000] [MSE loss: 0.036534] [Phy loss: 3.913666] [Total loss: 5.487589] [Lambda mse : 16.227642]\n",
      "[Epoch 5000/10000] [MSE loss: 0.037930] [Phy loss: 3.994036] [Total loss: 5.582629] [Lambda mse : 52.743710]\n",
      "[Epoch 5100/10000] [MSE loss: 0.058148] [Phy loss: 3.965495] [Total loss: 5.909774] [Lambda mse : 17.008331]\n",
      "[Epoch 5200/10000] [MSE loss: 0.048158] [Phy loss: 4.215898] [Total loss: 6.850008] [Lambda mse : 19.401423]\n",
      "[Epoch 5300/10000] [MSE loss: 0.040590] [Phy loss: 3.921285] [Total loss: 5.292317] [Lambda mse : 18.442453]\n",
      "[Epoch 5400/10000] [MSE loss: 0.070295] [Phy loss: 3.913836] [Total loss: 5.893740] [Lambda mse : 20.110125]\n",
      "[Epoch 5500/10000] [MSE loss: 0.037534] [Phy loss: 3.719566] [Total loss: 4.852191] [Lambda mse : 19.507061]\n",
      "[Epoch 5600/10000] [MSE loss: 0.043126] [Phy loss: 3.864396] [Total loss: 5.594930] [Lambda mse : 13.903656]\n",
      "[Epoch 5700/10000] [MSE loss: 0.054283] [Phy loss: 3.809413] [Total loss: 5.465310] [Lambda mse : 12.919676]\n",
      "[Epoch 5800/10000] [MSE loss: 0.057908] [Phy loss: 3.896599] [Total loss: 5.789063] [Lambda mse : 15.299683]\n",
      "[Epoch 5900/10000] [MSE loss: 0.037209] [Phy loss: 3.813459] [Total loss: 4.999360] [Lambda mse : 24.131071]\n",
      "[Epoch 6000/10000] [MSE loss: 0.044903] [Phy loss: 3.562573] [Total loss: 5.013298] [Lambda mse : 12.969080]\n",
      "[Epoch 6100/10000] [MSE loss: 0.044005] [Phy loss: 3.830920] [Total loss: 5.018085] [Lambda mse : 15.239551]\n",
      "[Epoch 6200/10000] [MSE loss: 0.034674] [Phy loss: 3.850948] [Total loss: 5.936132] [Lambda mse : 19.365381]\n",
      "[Epoch 6300/10000] [MSE loss: 0.048482] [Phy loss: 3.723606] [Total loss: 4.681365] [Lambda mse : 8.774294]\n",
      "[Epoch 6400/10000] [MSE loss: 0.047082] [Phy loss: 3.719998] [Total loss: 5.195541] [Lambda mse : 19.021919]\n",
      "[Epoch 6500/10000] [MSE loss: 0.033661] [Phy loss: 3.760976] [Total loss: 5.224371] [Lambda mse : 48.539112]\n",
      "[Epoch 6600/10000] [MSE loss: 0.035682] [Phy loss: 3.614530] [Total loss: 4.702971] [Lambda mse : 27.018068]\n",
      "[Epoch 6700/10000] [MSE loss: 0.037219] [Phy loss: 3.937684] [Total loss: 5.913756] [Lambda mse : 22.508591]\n",
      "[Epoch 6800/10000] [MSE loss: 0.044601] [Phy loss: 3.773735] [Total loss: 5.334266] [Lambda mse : 50.796692]\n",
      "[Epoch 6900/10000] [MSE loss: 0.043710] [Phy loss: 3.940287] [Total loss: 5.525218] [Lambda mse : 14.469116]\n",
      "[Epoch 7000/10000] [MSE loss: 0.044462] [Phy loss: 3.863443] [Total loss: 5.657475] [Lambda mse : 11.420319]\n",
      "[Epoch 7100/10000] [MSE loss: 0.035533] [Phy loss: 3.565491] [Total loss: 4.689031] [Lambda mse : 10.566151]\n",
      "[Epoch 7200/10000] [MSE loss: 0.051086] [Phy loss: 3.890013] [Total loss: 5.681059] [Lambda mse : 26.054821]\n",
      "[Epoch 7300/10000] [MSE loss: 0.038088] [Phy loss: 3.598763] [Total loss: 4.753824] [Lambda mse : 15.979270]\n",
      "[Epoch 7400/10000] [MSE loss: 0.037090] [Phy loss: 3.777975] [Total loss: 5.149925] [Lambda mse : 19.564512]\n",
      "[Epoch 7500/10000] [MSE loss: 0.038733] [Phy loss: 3.840367] [Total loss: 5.223769] [Lambda mse : 25.103422]\n",
      "[Epoch 7600/10000] [MSE loss: 0.042191] [Phy loss: 3.714044] [Total loss: 5.119673] [Lambda mse : 20.642485]\n",
      "[Epoch 7700/10000] [MSE loss: 0.044463] [Phy loss: 3.800095] [Total loss: 5.473060] [Lambda mse : 13.007687]\n",
      "[Epoch 7800/10000] [MSE loss: 0.048573] [Phy loss: 3.833146] [Total loss: 5.069459] [Lambda mse : 11.570802]\n",
      "[Epoch 7900/10000] [MSE loss: 0.047414] [Phy loss: 3.824030] [Total loss: 5.251303] [Lambda mse : 33.546883]\n",
      "[Epoch 8000/10000] [MSE loss: 0.035358] [Phy loss: 3.677172] [Total loss: 4.654648] [Lambda mse : 15.673618]\n",
      "[Epoch 8100/10000] [MSE loss: 0.034562] [Phy loss: 3.736124] [Total loss: 5.532236] [Lambda mse : 25.084902]\n",
      "[Epoch 8200/10000] [MSE loss: 0.039023] [Phy loss: 3.756812] [Total loss: 5.092103] [Lambda mse : 49.474899]\n",
      "[Epoch 8300/10000] [MSE loss: 0.050695] [Phy loss: 3.750154] [Total loss: 5.589051] [Lambda mse : 20.822214]\n",
      "[Epoch 8400/10000] [MSE loss: 0.046873] [Phy loss: 3.638825] [Total loss: 5.048069] [Lambda mse : 17.851482]\n",
      "[Epoch 8500/10000] [MSE loss: 0.043555] [Phy loss: 3.539052] [Total loss: 4.750878] [Lambda mse : 24.749977]\n",
      "[Epoch 8600/10000] [MSE loss: 0.050195] [Phy loss: 3.796066] [Total loss: 5.432869] [Lambda mse : 13.116137]\n",
      "[Epoch 8700/10000] [MSE loss: 0.041787] [Phy loss: 3.588014] [Total loss: 5.072562] [Lambda mse : 18.627388]\n",
      "[Epoch 8800/10000] [MSE loss: 0.048266] [Phy loss: 3.701839] [Total loss: 5.172663] [Lambda mse : 13.172586]\n",
      "[Epoch 8900/10000] [MSE loss: 0.050000] [Phy loss: 3.675429] [Total loss: 5.069146] [Lambda mse : 17.089462]\n",
      "[Epoch 9000/10000] [MSE loss: 0.045967] [Phy loss: 3.642347] [Total loss: 5.044097] [Lambda mse : 18.889919]\n",
      "[Epoch 9100/10000] [MSE loss: 0.039241] [Phy loss: 3.483443] [Total loss: 4.823260] [Lambda mse : 17.334955]\n",
      "[Epoch 9200/10000] [MSE loss: 0.037324] [Phy loss: 3.875072] [Total loss: 5.479161] [Lambda mse : 19.038042]\n",
      "[Epoch 9300/10000] [MSE loss: 0.037694] [Phy loss: 3.595762] [Total loss: 4.972374] [Lambda mse : 14.909321]\n",
      "[Epoch 9400/10000] [MSE loss: 0.043680] [Phy loss: 3.597033] [Total loss: 4.761919] [Lambda mse : 16.828876]\n",
      "[Epoch 9500/10000] [MSE loss: 0.041449] [Phy loss: 3.715280] [Total loss: 5.637493] [Lambda mse : 23.270798]\n",
      "[Epoch 9600/10000] [MSE loss: 0.048158] [Phy loss: 3.802970] [Total loss: 5.301452] [Lambda mse : 12.133593]\n",
      "[Epoch 9700/10000] [MSE loss: 0.031767] [Phy loss: 3.632342] [Total loss: 4.803464] [Lambda mse : 19.050632]\n",
      "[Epoch 9800/10000] [MSE loss: 0.045013] [Phy loss: 3.619090] [Total loss: 5.079747] [Lambda mse : 19.512093]\n",
      "[Epoch 9900/10000] [MSE loss: 0.038061] [Phy loss: 3.523546] [Total loss: 4.596631] [Lambda mse : 26.146465]\n"
     ]
    }
   ],
   "source": [
    "APINN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Training Set :  0.42464492\n",
      "Physical Consistency:  0.64041924\n"
     ]
    }
   ],
   "source": [
    "nsamples = 500\n",
    "train_mean_y, train_std_y = APINN.uncertainity_estimate(APINN.train_x, nsamples, [APINN.Ymean, APINN.Ystd])\n",
    "\n",
    "train_predictions = torch.Tensor(train_mean_y).float().to(device)\n",
    "\n",
    "std_y_t = torch.Tensor(APINN.Ystd).float().to(device)\n",
    "mean_y_t = torch.Tensor(APINN.Ymean).float().to(device)\n",
    "std_x_t = torch.Tensor(APINN.Xstd).float().to(device)\n",
    "mean_x_t = torch.Tensor(APINN.Xmean).float().to(device)\n",
    "\n",
    "train_y = (APINN.train_y * std_y_t) + mean_y_t\n",
    "train_x = (APINN.train_x * std_x_t) + mean_x_t\n",
    "\n",
    "mse_train = ((train_predictions - train_y)**2).mean()\n",
    "phy_train = torch.mean(APINN.physics_loss(train_x, train_predictions, [0, 1], [0, 1])).detach().cpu().numpy()\n",
    "\n",
    "print(\"MSE on Training Set : \", mse_train.detach().cpu().numpy())\n",
    "print(\"Physical Consistency: \", phy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Test Set :  0.61379516\n",
      "Physical Consistency:  0.7096468\n"
     ]
    }
   ],
   "source": [
    "test_mean_y, test_std_y = APINN.uncertainity_estimate(APINN.test_x, nsamples, [APINN.Ymean, APINN.Ystd])\n",
    "\n",
    "test_predictions = torch.Tensor(test_mean_y).float().to(device)\n",
    "\n",
    "test_y = (APINN.test_y * std_y_t) + mean_y_t\n",
    "test_x = (APINN.test_x * std_x_t) + mean_x_t\n",
    "\n",
    "mse_test = ((test_predictions - test_y)**2).mean()\n",
    "phy_test = torch.mean(APINN.physics_loss(test_x, test_predictions, [0, 1], [0, 1])).detach().cpu().numpy()\n",
    "\n",
    "print(\"MSE on Test Set : \", mse_test.detach().cpu().numpy())\n",
    "print(\"Physical Consistency: \", phy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
